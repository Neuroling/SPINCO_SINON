{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33fb766f-3c56-47eb-ad2d-9aa4d5f78c81",
   "metadata": {},
   "source": [
    "# Summarize Performance per subject\n",
    "---------------------\n",
    "Author:  G.FragaGonzalez for SINON study\n",
    "\n",
    "Description: read individual Gorilla outputs, preprocesss, summarize (Correctness per level, block , noise, etc) and save summary per file\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a231fc-d742-4d46-91fc-e97c6ee56ee4",
   "metadata": {},
   "source": [
    "## Gather individual stats\n",
    "### Import libraries and define paths\n",
    "Use location of this file to define relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3eccad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2023-11-03\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "print(\"Today's date:\", today)\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# filepaths - Use current script path as reference \n",
    "thisScriptDir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "baseDir = thisScriptDir[:thisScriptDir.find(\"Scripts\")]\n",
    "dirinput = os.path.join(baseDir + 'Data', 'preprocessed','pilot_2','data_exp_116083-v2')\n",
    "diroutput = os.path.join(baseDir + 'Analysis', 'pilot_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a72f2e-d6bd-45a3-8a6c-7079535de67d",
   "metadata": {},
   "source": [
    "### Find files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6b73899b-fb97-47b7-8686-1e0993058b50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  87  valid files\n"
     ]
    }
   ],
   "source": [
    "validfiles = [\n",
    "    files for files in glob.glob(dirinput + '/**/**/*.csv', recursive=True) if \n",
    "    'gathered/Concat' not in files and re.search(r'\\d+\\.csv', files) and \n",
    "    (os.path.basename(files).startswith('2FC') or os.path.basename(files).startswith('PM'))\n",
    "]\n",
    "\n",
    "print('Found ', len(validfiles), ' valid files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76033702-6ab3-43f2-8828-71bb3d90b50a",
   "metadata": {},
   "source": [
    "### Preprocess (loop per file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fdc59733-79dd-4584-863d-667b6902ed59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1175, 84)\n",
      "Read table size:  (1171, 84)\n",
      "Read table size:  (1180, 84)\n",
      "Read table size:  (1177, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1173, 84)\n",
      "Read table size:  (1171, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (2350, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1174, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1947, 87)\n",
      "Read table size:  (1943, 87)\n",
      "Read table size:  (1939, 87)\n",
      "Read table size:  (1957, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1944, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1945, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1939, 87)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1175, 84)\n",
      "Read table size:  (1171, 84)\n",
      "Read table size:  (1180, 84)\n",
      "Read table size:  (1177, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1173, 84)\n",
      "Read table size:  (1171, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (2350, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1174, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1175, 84)\n",
      "Read table size:  (1171, 84)\n",
      "Read table size:  (1180, 84)\n",
      "Read table size:  (1177, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1173, 84)\n",
      "Read table size:  (1171, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (2350, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1174, 84)\n",
      "Read table size:  (1170, 84)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1947, 87)\n",
      "Read table size:  (1943, 87)\n",
      "Read table size:  (1939, 87)\n",
      "Read table size:  (1957, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1944, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1945, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1939, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1947, 87)\n",
      "Read table size:  (1943, 87)\n",
      "Read table size:  (1939, 87)\n",
      "Read table size:  (1957, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1944, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1945, 87)\n",
      "Read table size:  (1938, 87)\n",
      "Read table size:  (1939, 87)\n",
      "Created list with  87  preprocessed files\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "for fileinput in validfiles: \n",
    "    # read data\n",
    "    dat = pd.read_csv(fileinput)      \n",
    "    print('Read table size: ', dat.shape )\n",
    "\n",
    "    #Rename variable to use as subject id\n",
    "    dat.rename(columns={'Participant Private ID':'SubjectID'}, inplace=True)\n",
    "    dat.SubjectID = pd.Series(dat.SubjectID,dtype=\"object\")\n",
    "\n",
    "    # Keep only rows with trial response(always the one after 'audio play requested')\n",
    "    idx_resp = dat.index[(dat.Response == 'AUDIO PLAY REQUESTED')] + 1    \n",
    "    df = dat.iloc[idx_resp]\n",
    "    df = df[df.display.str.contains('trial_')]\n",
    "\n",
    "    # Replace Correct responses of 'miss' trials by NAs\n",
    "    df.loc[df['Timed Out']==1,'Correct'] = 'miss'\n",
    "    df.loc[df['Timed Out']==1,'Reaction Time'] = np.nan\n",
    "\n",
    "    #% Find the presented audio based on the columns indicating list\n",
    "    # First rename column with the list name (the exact column varies depending on  task, i.e., across csv files )\n",
    "    cols2search = [i for i,val in enumerate(df.columns.str.contains('counterbalance*')) if val] \n",
    "    colWithList = [cols2search[i] for i,val in enumerate(cols2search) if any(df.iloc[:,val].astype(str).str.contains('list*'))]      \n",
    "    df.insert(2,\"STIMLIST\",df.iloc[:,colWithList]) #Add conveniently named column with list info    \n",
    "\n",
    "    # % create a variable 'audio' with the filename of presented audio\n",
    "    audio=[]\n",
    "    for row in range(len(df)):\n",
    "        audio.append(df[df.iloc[row]['STIMLIST']].iloc[row])\n",
    "    df.insert(len(df.columns),'AUDIO',audio)           \n",
    "\n",
    "    # %% Extract trial type info  from the audiofilenames  \n",
    "    df.insert(2,\"TYPE\",df['AUDIO'].str.split('norm').str[0].str.split('_').str[0])\n",
    "\n",
    "    # RECODE Levels of degradation\n",
    "    df.insert(2,\"LV\",df['AUDIO'].str.split('norm').str[1].str.replace('_','',regex=True).str.replace('.wav','',regex=True)) # use string split from filenames\n",
    "    replace_map = {'-10db': 'L1','-7.5db': 'L2', '-5db': 'L3', '-2.5db': 'L4','0db': 'L5',\\\n",
    "                           '0.7p': 'L1','0.75p': 'L2', '0.8p': 'L3', '0.85p': 'L4','0.9p': 'L5'}\n",
    "    df['LV'] = df['LV'].map(replace_map)\n",
    "\n",
    "    # Some Renaming\n",
    "    df.rename(columns={'Trial Number': 'trial'}, inplace=True)    \n",
    "    df.rename(columns={'Task Name': 'task'}, inplace=True)    \n",
    "    df.rename(columns={'Reaction Time': 'RT'}, inplace=True)    \n",
    "    df.rename(columns={'Timed Out': 'Miss'}, inplace=True)    \n",
    "    df.rename(columns={'Correct': 'Correctness'}, inplace=True)    \n",
    "    df['task'] = df['task'].str.replace('SINON_task_','')\n",
    "\n",
    "    # some  formatting        \n",
    "    df['block'] = df['block'].astype('object')\n",
    "    df['LV'] = df['LV'].astype('object')\n",
    "    df['RT'] = df['RT'].astype('float64')\n",
    "    df['Correctness'] = df['Correctness'].astype('str')\n",
    "    df['Correctness'].replace({'1.0':'cor','0.0':'inc','miss':'miss'}, inplace=True)     \n",
    "\n",
    "    # RECODE blocks for clarity in plots (1 and 2 value only)\n",
    "    df['block'].replace({1:1,2.0:1,3.0:2,4.0:2}, inplace=True)     \n",
    "     \n",
    "    # Select columns to save \n",
    "    #--------------------------\n",
    "    df = df.loc[:,['SubjectID','task','STIMLIST', 'set','block','trial', 'AUDIO', 'LV','TYPE','Correctness','RT']]\n",
    "    #Reset index \n",
    "    df = df.reset_index(drop=True)\n",
    "     \n",
    "    # add to list\n",
    "    df_list.append(df)\n",
    "\n",
    "print('Created list with ' , len(df_list), ' preprocessed files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630387f-e7f6-4329-9739-34db2a83e102",
   "metadata": {},
   "source": [
    "#### Describe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a8d2e366-b1dd-47da-8af9-4765bdc887ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created list with  14  summary tables\n"
     ]
    }
   ],
   "source": [
    "summary_list = []\n",
    "for i, df in enumerate(df_list):    \n",
    "     \n",
    "    # Get number of trials per degradation level in a block (changes across tasks)\n",
    "    nblocks = 2 \n",
    "    nreps = len(df.trial.unique()) / (nblocks*len(df.LV.unique()) * len(df.TYPE.unique()))   \n",
    "    counts = df['TYPE'].value_counts()\n",
    "\n",
    "    # DESCRIPTIVE STATS per block, type and level (averaging trials)          \n",
    "    #----------------------------------------------------------------\n",
    "    vars2summarize = ['SubjectID','task', 'block', 'TYPE', 'LV','Correctness']\n",
    "\n",
    "    # Accuracy summary \n",
    "    df.groupby(vars2summarize)['trial'].agg(['count']).reset_index()\n",
    "    accu = df.groupby(vars2summarize)['trial'].agg(['count']).reset_index()\n",
    "    accu['propTrials'] = round(accu['count']/nreps,ndigits=2)\n",
    " \n",
    "    #Fix header (join by '-')\n",
    "    rts = df.groupby(vars2summarize)[['RT']].agg(['mean', 'std']).reset_index()\n",
    "    rts.columns  =  ['_'.join(i) if len(i[1]) else ''.join(i) for i in rts.columns.tolist() ]\n",
    "\n",
    "    summary = pd.merge(accu, rts, on=vars2summarize)\n",
    "\n",
    "    # % Expand with all combinations of the variables \n",
    "    unique_categories = [summary[col].unique() for col in vars2summarize]    \n",
    "    multiindex = pd.MultiIndex.from_product(unique_categories, names=vars2summarize)\n",
    "\n",
    "    # reindexing\n",
    "    summary = (summary\n",
    "                 .set_index(vars2summarize) \n",
    "                 .reindex(multiindex,fill_value= '')\n",
    "                 .reset_index())\n",
    "\n",
    "\n",
    "    summary['SubjectID'] = summary['SubjectID'].astype('object')\n",
    "    summary['block'] = summary['block'].astype('object')\n",
    "\n",
    "    \n",
    "    # add to list\n",
    "    summary_list.append(summary)\n",
    "    \n",
    "print('Created list with ', len(summary_list), ' summary tables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed753d-ec42-407a-9c7d-1be3781e353f",
   "metadata": {},
   "source": [
    "### Concatenate and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ca5e48c4-1b10-45c2-820b-65032edfbf23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated all into a data frame of dimensions: (800, 10)\n",
      "saved in  V:\\Projects\\Spinco\\SINON\\Data\\analysis\\Gathered_summary_long.csv\n"
     ]
    }
   ],
   "source": [
    "concat_df = pd.concat(summary_list, ignore_index=True)\n",
    "print('Concatenated all into a data frame of dimensions:', concat_df.shape)\n",
    "\n",
    "# round numeric \n",
    "concat_df = concat_df.round(4)\n",
    "\n",
    "# Save the concatenated DataFrame to a new CSV file\n",
    "concat_df.to_csv(os.path.join(diroutput,'Gathered_summary_long.csv'), index=False)\n",
    "print('saved in ', os.path.join(diroutput,'Gathered_summary_long.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spyder-env] *",
   "language": "python",
   "name": "conda-env-spyder-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
